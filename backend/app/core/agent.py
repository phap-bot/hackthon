"""
AI Agent using Ollama for itinerary generation
"""
from __future__ import annotations
import json
from typing import Any, Dict, List
import os

# Try to use Ollama, fallback to simple template if not available
try:
    from langchain_community.llms import Ollama
    from langchain_core.prompts import PromptTemplate
    
    # Initialize Ollama (running on localhost:11434)
    OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    llm = Ollama(model="mistral", base_url=OLLAMA_HOST, temperature=0.7)
    OLLAMA_AVAILABLE = True
except ImportError:
    print("âš ï¸ Ollama not available, using template-based generation")
    OLLAMA_AVAILABLE = False
    llm = None

PROMPT_TMPL = """
Báº¡n lÃ  chuyÃªn gia du lá»‹ch Viá»‡t Nam ðŸ‡»ðŸ‡³.
HÃ£y táº¡o lá»‹ch trÃ¬nh {days} ngÃ y táº¡i {region}, ngÃ¢n sÃ¡ch {budget},
phong cÃ¡ch {theme}, phÆ°Æ¡ng tiá»‡n {transport}, dÃ nh cho {people} ngÆ°á»i.

Dá»¯ liá»‡u Ä‘á»‹a Ä‘iá»ƒm gáº§n Ä‘Ã¢y (Ä‘Ã£ lá»c theo khoáº£ng cÃ¡ch & loáº¡i hÃ¬nh):
{nearby_places}

YÃŠU Cáº¦U:
- Láº­p lá»‹ch trÃ¬nh theo tá»«ng ngÃ y, má»—i ngÃ y 5â€“8 hoáº¡t Ä‘á»™ng (thá»i gian, Ä‘á»‹a Ä‘iá»ƒm, mÃ´ táº£ ngáº¯n).
- Æ¯u tiÃªn cÃ¡c Ä‘á»‹a Ä‘iá»ƒm phÃ¹ há»£p theme vÃ  ngÃ¢n sÃ¡ch.
- Di chuyá»ƒn há»£p lÃ½ theo khoáº£ng cÃ¡ch (trÃ¡nh zigzag).
- Tráº£ vá» JSON **há»£p lá»‡** theo Ä‘Ãºng schema sau:

{{
  "overview": "TÃ³m táº¯t chuyáº¿n Ä‘i trong 1â€“2 dÃ²ng",
  "schedule": [
    {{
      "day": 1,
      "title": "TÃªn ngÃ y/Ä‘iá»ƒm nháº¥n",
      "activities": [
        {{"time": "08:00", "place": "TÃªn Ä‘á»‹a Ä‘iá»ƒm", "desc": "MÃ´ táº£ ngáº¯n"}}
      ]
    }}
  ],
  "total_cost_estimate": "Æ°á»›c lÆ°á»£ng chi phÃ­ (VND)"
}}
"""


def _extract_json(text: str) -> Dict[str, Any]:
    """Extract JSON from model output"""
    start = text.find("{")
    if start == -1:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y JSON trong output")
    snippet = text[start:]
    end = snippet.rfind("}")
    if end != -1:
        snippet = snippet[:end + 1]
    return json.loads(snippet)


def generate_itinerary_template(user_prefs: Dict[str, Any], nearby_places: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Generate itinerary using template-based approach (fallback)
    """
    days = user_prefs.get("days", 3)
    region = user_prefs.get("region", "Unknown")
    
    # Build simple schedule
    schedule = []
    for day in range(1, days + 1):
        activities = [
            {"time": "08:00", "place": f"Hoáº¡t Ä‘á»™ng buá»•i sÃ¡ng ngÃ y {day}", "desc": "KhÃ¡m phÃ¡ buá»•i sÃ¡ng"},
            {"time": "12:00", "place": f"Ä‚n trÆ°a ngÃ y {day}", "desc": "ThÆ°á»Ÿng thá»©c áº©m thá»±c Ä‘á»‹a phÆ°Æ¡ng"},
            {"time": "14:00", "place": f"Hoáº¡t Ä‘á»™ng buá»•i chiá»u ngÃ y {day}", "desc": "Tham quan chiá»u"},
            {"time": "18:00", "place": f"Ä‚n tá»‘i ngÃ y {day}", "desc": "DÃ¹ng bá»¯a tá»‘i"},
        ]
        schedule.append({
            "day": day,
            "title": f"NgÃ y {day} - {region}",
            "activities": activities
        })
    
    return {
        "overview": f"Chuyáº¿n Ä‘i {days} ngÃ y táº¡i {region} vá»›i ngÃ¢n sÃ¡ch {user_prefs.get('budget', 'Trung bÃ¬nh')}",
        "schedule": schedule,
        "total_cost_estimate": "2.000.000Ä‘"
    }


def generate_itinerary(user_prefs: Dict[str, Any], nearby_places: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Generate itinerary using AI (Ollama if available, otherwise template)
    """
    if not OLLAMA_AVAILABLE:
        return generate_itinerary_template(user_prefs, nearby_places)
    
    try:
        from langchain_core.prompts import PromptTemplate as LangPromptTemplate
        prompt = LangPromptTemplate(
            template=PROMPT_TMPL,
            input_variables=["days", "region", "budget", "theme", "transport", "people", "nearby_places"],
        )
        
        full_prompt = prompt.format(
            days=user_prefs.get("days"),
            region=user_prefs.get("region"),
            budget=user_prefs.get("budget"),
            theme=user_prefs.get("theme"),
            transport=user_prefs.get("transport"),
            people=user_prefs.get("people"),
            nearby_places=json.dumps(nearby_places, ensure_ascii=False, indent=2),
        )

        raw = llm.invoke(full_prompt)
        return _extract_json(raw)
    except Exception as e:
        print(f"Error using Ollama: {e}")
        return generate_itinerary_template(user_prefs, nearby_places)

